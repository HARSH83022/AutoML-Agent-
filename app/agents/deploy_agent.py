# # app/agents/deploy_agent.py
# from app.utils.llm_clients import llm_generate_json
# import json
# import os

# def generate_deploy_scaffold(run_id, model_path, transformer_path, request_schema):
#     prompt = f"""
# You are a deployment template generator. Produce JSON:
# {{ "serve_py": "...", "dockerfile": "..." }}
# The serve_py should be a FastAPI app that loads model at '{model_path}' and transformer at '{transformer_path}', exposes POST /predict and validates request against schema {json.dumps(request_schema)}.
# Return JSON only.
# """
#     j = llm_generate_json(prompt)
#     if j and "serve_py" in j and "dockerfile" in j:
#         serve = j["serve_py"]
#         dockerfile = j["dockerfile"]
#     else:
#         # fallback safe templates
#         serve = f'''
# from fastapi import FastAPI
# from pydantic import BaseModel
# import joblib, pandas as pd

# class Req(BaseModel):
# {chr(10).join([f"    {k}: {('int' if v=='int' else 'float')}" for k,v in request_schema.items()])}

# app = FastAPI()
# model = joblib.load("{model_path}")
# transformer = joblib.load("{transformer_path}")

# @app.post("/predict")
# def predict(payload: Req):
#     df = pd.DataFrame([payload.dict()])
#     X = transformer.transform(df)
#     pred = model.predict(X)
#     return {{"prediction": int(pred[0])}}
# '''
#         dockerfile = f'''
# FROM python:3.10-slim
# WORKDIR /app
# COPY artifacts /app/artifacts
# RUN pip install fastapi uvicorn joblib pandas scikit-learn
# CMD ["uvicorn","artifacts.{run_id}_serve:app","--host","0.0.0.0","--port","8080"]
# '''
#     os.makedirs("artifacts", exist_ok=True)
#     serve_path = f"artifacts/{run_id}_serve.py"
#     docker_path = f"artifacts/{run_id}_Dockerfile"
#     with open(serve_path, "w") as f:
#         f.write(serve)
#     with open(docker_path, "w") as f:
#         f.write(dockerfile)
#     return {"serve": serve_path, "dockerfile": docker_path}

# app/agents/deploy_agent.py
import os, json
from app.utils.llm_clients import llm_generate_json
from app.utils.run_logger import agent_log

ARTIFACT_DIR = "artifacts"
os.makedirs(ARTIFACT_DIR, exist_ok=True)

def generate_deploy_scaffold(run_id, model_path, transformer_path, request_schema=None):
    """Generate complete deployment artifacts including FastAPI, Docker, platform configs, and SDK examples"""
    agent_log(run_id, "[deploy_agent] Generating deployment artifacts", agent="deploy_agent")
    
    # Generate FastAPI serving script
    serve_code = f'''"""
FastAPI Model Serving Script
Auto-generated by AutoML Platform
"""
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import joblib
import pandas as pd
import numpy as np
from typing import Dict, Any, List

app = FastAPI(
    title="AutoML Model API",
    description="Deployed ML model serving endpoint",
    version="1.0.0"
)

# Load model and transformer
model = joblib.load("{model_path}")
transformer = joblib.load("{transformer_path}")

class PredictionRequest(BaseModel):
    features: Dict[str, Any]
    
class PredictionResponse(BaseModel):
    prediction: float
    probability: float = None

@app.get("/")
def root():
    return {{"message": "AutoML Model API", "status": "healthy"}}

@app.get("/health")
def health():
    return {{"status": "healthy", "model_loaded": model is not None}}

@app.post("/predict", response_model=PredictionResponse)
def predict(request: PredictionRequest):
    try:
        # Convert to DataFrame
        df = pd.DataFrame([request.features])
        
        # Transform features
        X = transformer.transform(df)
        
        # Make prediction
        prediction = model.predict(X)[0]
        
        # Get probability if available
        probability = None
        if hasattr(model, "predict_proba"):
            proba = model.predict_proba(X)[0]
            probability = float(proba[1]) if len(proba) == 2 else float(max(proba))
        
        return PredictionResponse(
            prediction=float(prediction),
            probability=probability
        )
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

@app.post("/batch_predict")
def batch_predict(requests: List[Dict[str, Any]]):
    try:
        df = pd.DataFrame(requests)
        X = transformer.transform(df)
        predictions = model.predict(X)
        
        results = []
        for i, pred in enumerate(predictions):
            result = {{"prediction": float(pred)}}
            if hasattr(model, "predict_proba"):
                proba = model.predict_proba(X)[i]
                result["probability"] = float(proba[1]) if len(proba) == 2 else float(max(proba))
            results.append(result)
        
        return {{"predictions": results}}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8080)
'''
    
    # Generate Dockerfile
    dockerfile = f'''FROM python:3.10-slim

WORKDIR /app

# Copy artifacts
COPY {ARTIFACT_DIR} /app/{ARTIFACT_DIR}

# Install dependencies
RUN pip install --no-cache-dir \\
    fastapi==0.104.1 \\
    uvicorn[standard]==0.24.0 \\
    joblib==1.3.2 \\
    pandas==2.1.3 \\
    numpy==1.26.2 \\
    scikit-learn==1.3.2 \\
    xgboost==2.0.2 \\
    lightgbm==4.1.0

# Expose port
EXPOSE 8080

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\
    CMD python -c "import requests; requests.get('http://localhost:8080/health')"

# Run server
CMD ["uvicorn", "{ARTIFACT_DIR}.{run_id}_serve:app", "--host", "0.0.0.0", "--port", "8080"]
'''
    
    # Generate Render config
    render_yaml = f'''services:
  - type: web
    name: automl-model-{run_id}
    env: docker
    dockerfilePath: ./{ARTIFACT_DIR}/{run_id}_Dockerfile
    envVars:
      - key: PORT
        value: 8080
    healthCheckPath: /health
'''
    
    # Generate Railway config
    railway_json = {
        "build": {
            "builder": "DOCKERFILE",
            "dockerfilePath": f"{ARTIFACT_DIR}/{run_id}_Dockerfile"
        },
        "deploy": {
            "startCommand": f"uvicorn {ARTIFACT_DIR}.{run_id}_serve:app --host 0.0.0.0 --port $PORT",
            "restartPolicyType": "ON_FAILURE",
            "healthcheckPath": "/health"
        }
    }
    
    # Generate Vercel config
    vercel_json = {
        "builds": [{
            "src": f"{ARTIFACT_DIR}/{run_id}_serve.py",
            "use": "@vercel/python"
        }],
        "routes": [{
            "src": "/(.*)",
            "dest": f"{ARTIFACT_DIR}/{run_id}_serve.py"
        }]
    }
    
    # Generate Python SDK
    python_sdk = f'''"""
Python SDK for AutoML Model API
"""
import requests
from typing import Dict, Any, List

class AutoMLClient:
    def __init__(self, base_url: str, api_key: str = None):
        self.base_url = base_url.rstrip("/")
        self.api_key = api_key
        self.headers = {{}}
        if api_key:
            self.headers["Authorization"] = f"Bearer {{api_key}}"
    
    def predict(self, features: Dict[str, Any]) -> Dict[str, Any]:
        """Make a single prediction"""
        response = requests.post(
            f"{{self.base_url}}/predict",
            json={{"features": features}},
            headers=self.headers
        )
        response.raise_for_status()
        return response.json()
    
    def batch_predict(self, features_list: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Make batch predictions"""
        response = requests.post(
            f"{{self.base_url}}/batch_predict",
            json=features_list,
            headers=self.headers
        )
        response.raise_for_status()
        return response.json()
    
    def health(self) -> Dict[str, Any]:
        """Check API health"""
        response = requests.get(f"{{self.base_url}}/health")
        response.raise_for_status()
        return response.json()

# Example usage
if __name__ == "__main__":
    client = AutoMLClient("http://localhost:8080")
    
    # Single prediction
    result = client.predict({{"feature1": 1.0, "feature2": 2.0}})
    print(f"Prediction: {{result}}")
    
    # Batch prediction
    batch_results = client.batch_predict([
        {{"feature1": 1.0, "feature2": 2.0}},
        {{"feature1": 3.0, "feature2": 4.0}}
    ])
    print(f"Batch predictions: {{batch_results}}")
'''
    
    # Generate JavaScript SDK
    js_sdk = f'''/**
 * JavaScript SDK for AutoML Model API
 */
class AutoMLClient {{
    constructor(baseUrl, apiKey = null) {{
        this.baseUrl = baseUrl.replace(/\\/$/, '');
        this.apiKey = apiKey;
    }}
    
    async predict(features) {{
        const response = await fetch(`${{this.baseUrl}}/predict`, {{
            method: 'POST',
            headers: {{
                'Content-Type': 'application/json',
                ...(this.apiKey && {{ 'Authorization': `Bearer ${{this.apiKey}}` }})
            }},
            body: JSON.stringify({{ features }})
        }});
        
        if (!response.ok) {{
            throw new Error(`HTTP error! status: ${{response.status}}`);
        }}
        
        return await response.json();
    }}
    
    async batchPredict(featuresList) {{
        const response = await fetch(`${{this.baseUrl}}/batch_predict`, {{
            method: 'POST',
            headers: {{
                'Content-Type': 'application/json',
                ...(this.apiKey && {{ 'Authorization': `Bearer ${{this.apiKey}}` }})
            }},
            body: JSON.stringify(featuresList)
        }});
        
        if (!response.ok) {{
            throw new Error(`HTTP error! status: ${{response.status}}`);
        }}
        
        return await response.json();
    }}
    
    async health() {{
        const response = await fetch(`${{this.baseUrl}}/health`);
        if (!response.ok) {{
            throw new Error(`HTTP error! status: ${{response.status}}`);
        }}
        return await response.json();
    }}
}}

// Example usage
const client = new AutoMLClient('http://localhost:8080');

// Single prediction
client.predict({{ feature1: 1.0, feature2: 2.0 }})
    .then(result => console.log('Prediction:', result))
    .catch(error => console.error('Error:', error));

// Batch prediction
client.batchPredict([
    {{ feature1: 1.0, feature2: 2.0 }},
    {{ feature1: 3.0, feature2: 4.0 }}
])
    .then(results => console.log('Batch predictions:', results))
    .catch(error => console.error('Error:', error));
'''
    
    # Generate curl examples
    curl_examples = f'''# AutoML Model API - curl Examples

# Health check
curl http://localhost:8080/health

# Single prediction
curl -X POST http://localhost:8080/predict \\
  -H "Content-Type: application/json" \\
  -d '{{"features": {{"feature1": 1.0, "feature2": 2.0}}}}'

# Batch prediction
curl -X POST http://localhost:8080/batch_predict \\
  -H "Content-Type: application/json" \\
  -d '[{{"feature1": 1.0, "feature2": 2.0}}, {{"feature1": 3.0, "feature2": 4.0}}]'

# With authentication (if enabled)
curl -X POST http://localhost:8080/predict \\
  -H "Content-Type: application/json" \\
  -H "Authorization: Bearer YOUR_API_KEY" \\
  -d '{{"features": {{"feature1": 1.0, "feature2": 2.0}}}}'
'''
    
    # Save all files
    files = {
        f"{run_id}_serve.py": serve_code,
        f"{run_id}_Dockerfile": dockerfile,
        f"{run_id}_render.yaml": render_yaml,
        f"{run_id}_railway.json": json.dumps(railway_json, indent=2),
        f"{run_id}_vercel.json": json.dumps(vercel_json, indent=2),
        f"{run_id}_sdk.py": python_sdk,
        f"{run_id}_sdk.js": js_sdk,
        f"{run_id}_curl_examples.sh": curl_examples
    }
    
    saved_paths = {}
    for filename, content in files.items():
        path = os.path.join(ARTIFACT_DIR, filename)
        with open(path, "w", encoding="utf-8") as f:
            f.write(content)
        saved_paths[filename] = path
        agent_log(run_id, f"[deploy_agent] Generated {filename}", agent="deploy_agent")
    
    agent_log(run_id, "[deploy_agent] Deployment artifacts complete", agent="deploy_agent")
    
    return {
        "serve": saved_paths[f"{run_id}_serve.py"],
        "dockerfile": saved_paths[f"{run_id}_Dockerfile"],
        "render_config": saved_paths[f"{run_id}_render.yaml"],
        "railway_config": saved_paths[f"{run_id}_railway.json"],
        "vercel_config": saved_paths[f"{run_id}_vercel.json"],
        "python_sdk": saved_paths[f"{run_id}_sdk.py"],
        "javascript_sdk": saved_paths[f"{run_id}_sdk.js"],
        "curl_examples": saved_paths[f"{run_id}_curl_examples.sh"]
    }
